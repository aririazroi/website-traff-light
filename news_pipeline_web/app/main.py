"""
Main CLI entrypoint for the website news pipeline.

Fetches articles, scores them, samples 3 from each color bin,
and outputs articles.js for the website.

Usage:
    python -m app.main [--out OUTPUT_PATH] [--per-bin N]
"""
import argparse
import json
import os
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import List

from .config import load_config, Config
from .gnews_client import GNewsClient
from .models import Article
from .scoring import load_scorer, validate_score
from .mapping import score_to_traffic_light
from .sampling import sample_by_color, get_bin_stats


def run_pipeline(config: Config) -> List[Article]:
    """
    Execute the full pipeline: fetch -> score -> map.
    """
    print(f"[Pipeline] Starting with query='{config.query}', max={config.max_results}")
    
    # 1. Fetch articles from GNews
    print("[Pipeline] Fetching articles from GNews...")
    client = GNewsClient(config)
    articles = client.fetch_articles()
    print(f"[Pipeline] Fetched {len(articles)} articles")
    
    if not articles:
        print("[Pipeline] No articles fetched, exiting early")
        return []
    
    # 2. Load the scorer
    print(f"[Pipeline] Loading scorer from module: {config.scorer_module}")
    try:
        scorer = load_scorer(config)
    except (ImportError, AttributeError) as e:
        print(f"[Pipeline] ERROR: Failed to load scorer: {e}")
        print("[Pipeline] Make sure SCORER_MODULE points to a module with score(title, content) -> int")
        sys.exit(1)
    
    # 3. Score each article and map to traffic light
    print("[Pipeline] Scoring articles...")
    for i, article in enumerate(articles):
        try:
            # Use raw content (strip HTML for scoring)
            raw_content = article.content.replace("<p>", "").replace("</p>", " ").strip()
            raw_score = scorer(article.title, raw_content)
            score = validate_score(raw_score)
            color = score_to_traffic_light(score, config)
            
            article.misleadingScore = score
            article.trafficLightStatus = color
            
            print(f"  [{i+1}/{len(articles)}] score={score}, color={color} | {article.title[:50]}...")
        except Exception as e:
            print(f"  [{i+1}/{len(articles)}] ERROR scoring: {e}")
            article.misleadingScore = 50
            article.trafficLightStatus = "yellow"
    
    return articles


def sample_articles(articles: List[Article], per_bin: int) -> List[Article]:
    """
    Sample N articles from each color bin.
    """
    stats = get_bin_stats(articles)
    print(f"[Pipeline] Article distribution: green={stats['green']}, yellow={stats['yellow']}, red={stats['red']}")
    
    sampled = sample_by_color(articles, per_bin=per_bin)
    print(f"[Pipeline] Sampled {len(sampled)} articles ({per_bin} per bin)")
    
    # Re-assign sequential IDs after sampling
    for i, article in enumerate(sampled, start=1):
        article.id = i
    
    return sampled


def write_articles_js(articles: List[Article], output_path: str) -> None:
    """
    Write articles as JavaScript file for website consumption.
    """
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Build JavaScript content
    articles_data = [article.to_dict() for article in articles]
    
    # Format as JavaScript (not JSON) for direct inclusion
    js_content = f"""// Auto-generated by news_pipeline_web
// Generated: {datetime.now(timezone.utc).isoformat()}
// Articles: {len(articles)}

const articles = {json.dumps(articles_data, indent=4, ensure_ascii=False)};
"""
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(js_content)
    
    print(f"[Pipeline] Wrote {len(articles)} articles to {output_path}")


def write_articles_json(articles: List[Article], output_path: str) -> None:
    """
    Write articles as JSON file (alternative format).
    """
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output = {
        "lastUpdated": datetime.now(timezone.utc).isoformat(),
        "articles": [article.to_dict() for article in articles],
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"[Pipeline] Wrote {len(articles)} articles to {output_path}")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Fetch GNews articles, score, sample by color, output for website."
    )
    parser.add_argument(
        "--out", 
        dest="output_path",
        help="Output file path (overrides OUTPUT_PATH env var)"
    )
    parser.add_argument(
        "--per-bin",
        dest="per_bin",
        type=int,
        default=3,
        help="Number of articles to sample from each color bin (default: 3)"
    )
    parser.add_argument(
        "--format",
        dest="format",
        choices=["js", "json"],
        default="js",
        help="Output format: 'js' for articles.js, 'json' for articles.json (default: js)"
    )
    args = parser.parse_args()
    
    # Load config
    try:
        config = load_config()
    except ValueError as e:
        print(f"[Pipeline] Configuration error: {e}")
        sys.exit(1)
    
    # Override output path if provided via CLI
    if args.output_path:
        config.output_path = args.output_path
    
    # Run the pipeline
    articles = run_pipeline(config)
    
    if not articles:
        print("[Pipeline] No articles to process")
        sys.exit(0)
    
    # Sample from each bin
    sampled = sample_articles(articles, per_bin=args.per_bin)
    
    # Write output
    if args.format == "js":
        write_articles_js(sampled, config.output_path)
    else:
        write_articles_json(sampled, config.output_path)
    
    print("[Pipeline] Done!")


if __name__ == "__main__":
    main()
